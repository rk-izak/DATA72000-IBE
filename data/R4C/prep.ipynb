{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep. of R4C Data\n",
    "### This notebook contains the extraction and processing of data from R4C raw files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "RAW_DIR = Path('./raw')\n",
    "CLEAN_DIR = Path('./clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure clean directory exists\n",
    "CLEAN_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import getpass\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import getpass\n",
    "\n",
    "def set_api_key(file_path: str, api_name: str):\n",
    "  if file_path == \"\":\n",
    "        file_path = \"/Users/radoslawizak/Desktop/MSc Data Science/Research project/keys.json\"\n",
    "  # Current APIs: \"OpenAI\", \"Anthropic\", \"LangChain\", \"Tavily\"\n",
    "  with open(file_path, 'r') as file:\n",
    "      api_keys = json.load(file)\n",
    "        \n",
    "  for api in api_keys:\n",
    "      if api[\"name\"].lower() == api_name.lower():\n",
    "          api_key = api[\"key\"]\n",
    "          env_var_name = f\"{api_name.upper()}_API_KEY\"\n",
    "          os.environ[env_var_name] = api_key\n",
    "\n",
    "          if api_name.lower() == \"langchain\":\n",
    "              os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "          if not os.environ.get(env_var_name):\n",
    "              os.environ[env_var_name] = getpass.getpass(f\"{env_var_name}: \")\n",
    "          break\n",
    "  else:\n",
    "      print(f\"API name '{api_name}' not found in the provided file.\")\n",
    "file_path = \"/Users/radoslawizak/Desktop/MSc Data Science/Research project/keys.json\"\n",
    "\n",
    "### LLMs\n",
    "set_api_key(\"\", api_name=\"OpenAI\")\n",
    "set_api_key(\"\", api_name=\"Anthropic\")\n",
    "\n",
    "# Tracing\n",
    "set_api_key(\"\", api_name=\"LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLMs (assumes API keys have been set -> check utils)\n",
    "openai_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "anthropic_llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Read a JSON file and return its contents.\n",
    "\n",
    "    Args:\n",
    "    filename (str): Name of the file to read.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Any]: Contents of the JSON file.\n",
    "    \"\"\"\n",
    "    with open(RAW_DIR / filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data: Any, filename: str):\n",
    "    \"\"\"\n",
    "    Save data as a JSON file in the clean directory.\n",
    "\n",
    "    Args:\n",
    "    data (Any): Data to be saved.\n",
    "    filename (str): Name of the file to save.\n",
    "    \"\"\"\n",
    "    with open(CLEAN_DIR / filename, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "def process_dev_csf(dev_csf: Dict[str, Any]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Process the dev_csf file to extract unique evidence sentences.\n",
    "\n",
    "    Args:\n",
    "    dev_csf (Dict[str, Any]): The contents of the dev_csf file.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, List[str]]: A dictionary with IDs as keys and lists of unique evidence sentences as values.\n",
    "    \"\"\"\n",
    "    processed_data = {}\n",
    "    for key, value in dev_csf.items():\n",
    "        evidence_set = set()\n",
    "        for item in value:\n",
    "            for entry in item:\n",
    "                evidence = entry[2]\n",
    "                evidence_set.add(\" \".join(evidence))\n",
    "        processed_data[key] = list(evidence_set)\n",
    "    return processed_data\n",
    "\n",
    "def generate_claim(question: str, answer: str, llm: Any) -> str:\n",
    "    \"\"\"\n",
    "    Generate a claim from the question and answer using the specified LLM.\n",
    "\n",
    "    Args:\n",
    "    question (str): The original question.\n",
    "    answer (str): The original answer.\n",
    "    llm (Any): The language model to use.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated claim.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"answer\"],\n",
    "        template=\"Convert the following question and answer into a single and concise sentence claim. Do NOT introduce any new information, just paraphrase in simple language. Do NOT return ANY other additional text, just the claim:\\nQuestion: {question}\\nAnswer: {answer}\\nClaim:\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    return chain.run(question=question, answer=answer).strip()\n",
    "\n",
    "def generate_explanation(evidence: List[str], claim: str, llm: Any) -> str:\n",
    "    \"\"\"\n",
    "    Generate an explanation for the claim using the evidence and the specified LLM.\n",
    "\n",
    "    Args:\n",
    "    evidence (List[str]): The list of evidence sentences.\n",
    "    claim (str): The claim to explain.\n",
    "    llm (Any): The language model to use.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated explanation.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"evidence\", \"claim\"],\n",
    "        template=\"Using the following evidence, create a short and concise explanation (1-3 sentences) for the claim. Use simple and plain language. Do NOT introduce any new information, just combine what is given. Keep paraphrasing to a minimum. Do NOT return ANY other additional text, just the explanation:\\nEvidence: {evidence}\\nClaim: {claim}\\nExplanation:\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    return chain.run(evidence=\"\\n\".join(evidence), claim=claim).strip()\n",
    "\n",
    "def process_evidence(evidence: List[List[str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Process the evidence to combine text without headlines.\n",
    "\n",
    "    Args:\n",
    "    evidence (List[List[str]]): The original evidence list.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Processed evidence list.\n",
    "    \"\"\"\n",
    "    return [' '.join(item[1]) for item in evidence]\n",
    "\n",
    "def remove_duplicate_info(evidence: List[str], llm: Any) -> List[str]:\n",
    "    \"\"\"\n",
    "    Remove informationally duplicate entries from the evidence list using an LLM.\n",
    "\n",
    "    Args:\n",
    "    evidence (List[str]): The original evidence list.\n",
    "    llm (Any): The language model to use.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Evidence list with informationally duplicate entries removed.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"evidence\"],\n",
    "        template=\"Remove any informationally duplicate entries from the following list, including paraphrases or similar information. Do NOT change the content under any circumstances, only remove. Return only the unique informational content without any leading dashes or bullet points. Do NOT return ANY other additional text, just the evidence:\\n{evidence}\\nUnique information:\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    unique_info = chain.run(evidence=\"\\n\".join(evidence)).strip().split(\"\\n\")\n",
    "    return [info.strip().lstrip('- ') for info in unique_info if info.strip()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input files\n",
    "dev_csf = read_json('dev_csf.json')\n",
    "hotpot = read_json('hotpot_dev_fullwiki_v1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dev_csf\n",
    "processed_dev_csf = process_dev_csf(dev_csf)\n",
    "\n",
    "# Create full data\n",
    "full_data = {}\n",
    "\n",
    "# Get the intersection of IDs from both datasets\n",
    "common_ids = list(set(processed_dev_csf.keys()) & set(item['_id'] for item in hotpot))\n",
    "\n",
    "# Randomly sample 200 IDs (or all if less than 100)\n",
    "sample_size = min(200, len(common_ids))\n",
    "sampled_ids = random.sample(common_ids, sample_size)\n",
    "\n",
    "# Initialize evidence knowledge bases\n",
    "evidence_kb = []\n",
    "evidence_golden_kb = []\n",
    "evidence_id_counter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/radoslawizak/Desktop/MSc Data Science/Research project/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/Users/radoslawizak/Desktop/MSc Data Science/Research project/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "for item in hotpot:\n",
    "    item_id = item['_id']\n",
    "    if item_id in sampled_ids:\n",
    "        # Randomly decide which model to use\n",
    "        model_used = random.choice([\"OpenAI\", \"Anthropic\"])\n",
    "        \n",
    "        # Select the appropriate LLM based on the random choice\n",
    "        llm = openai_llm if model_used == \"OpenAI\" else anthropic_llm\n",
    "\n",
    "        # Generate claim using the selected model\n",
    "        claim = generate_claim(item['question'], item['answer'], llm)\n",
    "        \n",
    "        # Process and remove informationally duplicate evidence using the same model\n",
    "        evidence_golden = remove_duplicate_info(processed_dev_csf[item_id], llm)\n",
    "        evidence = process_evidence(item['context'])\n",
    "        \n",
    "        # Generate explanation using the same model\n",
    "        explanation = generate_explanation(evidence_golden, claim, llm)\n",
    "        \n",
    "        # Create evidence structures with IDs\n",
    "        evidence_with_ids = []\n",
    "        for ev in evidence:\n",
    "            evidence_with_ids.append({\n",
    "                \"evidence_id\": evidence_id_counter,\n",
    "                \"description\": ev,\n",
    "            })\n",
    "            evidence_kb.append({\n",
    "                \"evidence_id\": evidence_id_counter,\n",
    "                \"description\": ev,\n",
    "            })\n",
    "            evidence_id_counter += 1\n",
    "        \n",
    "        evidence_golden_with_ids = []\n",
    "        for ev in evidence_golden:\n",
    "            evidence_golden_with_ids.append({\n",
    "                \"evidence_id\": evidence_id_counter,\n",
    "                \"description\": ev,\n",
    "                \"model_used\": model_used\n",
    "            })\n",
    "            evidence_golden_kb.append({\n",
    "                \"evidence_id\": evidence_id_counter,\n",
    "                \"description\": ev,\n",
    "                \"model_used\": model_used\n",
    "            })\n",
    "            evidence_id_counter += 1\n",
    "        \n",
    "        full_data[item_id] = {\n",
    "            'claim': claim,\n",
    "            'model_used': model_used,\n",
    "            'explanation': explanation,\n",
    "            'evidence_golden': evidence_golden_with_ids,\n",
    "            'evidence': evidence_with_ids\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing completed. Files saved in the clean directory:\n",
      "- full_data.json with 200 items\n",
      "- evidence_kb.json with 1972 items\n",
      "- evidence_golden_kb.json with 590 items\n"
     ]
    }
   ],
   "source": [
    "# Save full data\n",
    "save_json(full_data, 'full_data.json')\n",
    "\n",
    "# Save evidence knowledge bases\n",
    "save_json(evidence_kb, 'evidence_kb.json')\n",
    "save_json(evidence_golden_kb, 'evidence_golden_kb.json')\n",
    "\n",
    "print(f\"Data processing completed. Files saved in the clean directory:\")\n",
    "print(f\"- full_data.json with {len(full_data)} items\")\n",
    "print(f\"- evidence_kb.json with {len(evidence_kb)} items\")\n",
    "print(f\"- evidence_golden_kb.json with {len(evidence_golden_kb)} items\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
