{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep. of CIVIC Data\n",
    "\n",
    "### This notebook contains the extraction and processing of data from CIVIC raw files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup\n",
    "\n",
    "Import core libraries used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Define paths (currently relative, adjust as needed)\n",
    "RAW_DIR = Path('./raw')\n",
    "CLEAN_DIR = Path('./clean')\n",
    "CONTEXT_DIR = CLEAN_DIR / 'context'\n",
    "\n",
    "# Ensure clean directory exists\n",
    "CLEAN_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Used later on to speed-up the process, each function has its own dosctring for easier understanding and usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a TSV file and return a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    filename (str): Name of the file to read.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the file contents.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(RAW_DIR / filename, sep='\\t')\n",
    "\n",
    "def process_assertion(row: pd.Series, clinical_evidence: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single assertion row and return a dictionary with extracted information.\n",
    "\n",
    "    Args:\n",
    "    row (pd.Series): A row from the assertion DataFrame.\n",
    "    clinical_evidence (pd.DataFrame): DataFrame containing clinical evidence summaries.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Any]: Processed assertion data.\n",
    "    \"\"\"\n",
    "    evidence_ids = row['evidence_item_ids'].split(',')\n",
    "    evidence_list = []\n",
    "    \n",
    "    for eid in evidence_ids:\n",
    "        evidence = clinical_evidence[clinical_evidence['evidence_id'] == int(eid)]\n",
    "        if not evidence.empty:\n",
    "            evidence_list.append({\n",
    "                'evidence_id': int(eid),\n",
    "                'description': evidence['evidence_statement'].iloc[0]\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        'claim': row['assertion_summary'],\n",
    "        'explanation': row['assertion_description'],\n",
    "        'evidence': evidence_list\n",
    "    }\n",
    "\n",
    "def save_json(data: Any, filename: str):\n",
    "    \"\"\"\n",
    "    Save data as a JSON file in the clean directory.\n",
    "\n",
    "    Args:\n",
    "    data (Any): Data to be saved.\n",
    "    filename (str): Name of the file to save.\n",
    "    \"\"\"\n",
    "    with open(CLEAN_DIR / filename, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "def create_missing_evidence_file(baseline_data: List[Dict[str, Any]], deletion_probability: float = 0.3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create a new dataset with some evidence items randomly deleted from each entry.\n",
    "\n",
    "    Args:\n",
    "    baseline_data (List[Dict[str, Any]]): The original processed assertions data.\n",
    "    deletion_probability (float): Probability of deleting an evidence item (default: 0.3).\n",
    "\n",
    "    Returns:\n",
    "    List[Dict[str, Any]]: New dataset with some evidence items moved to 'missing_evidence'.\n",
    "    \"\"\"\n",
    "    missing_evidence_data = []\n",
    "\n",
    "    for entry in baseline_data:\n",
    "        new_entry = entry.copy()\n",
    "        original_evidence = new_entry['evidence']\n",
    "        new_evidence = []\n",
    "        missing_evidence = []\n",
    "\n",
    "        # Determine number of items to potentially delete\n",
    "        num_evidence = len(original_evidence)\n",
    "        if num_evidence > 1:\n",
    "            max_to_delete = random.randint(1, num_evidence - 1)\n",
    "        else:\n",
    "            max_to_delete = 0\n",
    "\n",
    "        # Randomly select items to keep or delete\n",
    "        for item in original_evidence:\n",
    "            if len(missing_evidence) < max_to_delete and random.random() < deletion_probability:\n",
    "                missing_evidence.append(item)\n",
    "            else:\n",
    "                new_evidence.append(item)\n",
    "\n",
    "        # Ensure at least one item is deleted if possible\n",
    "        if len(missing_evidence) == 0 and len(new_evidence) > 1:\n",
    "            item_to_move = random.choice(new_evidence)\n",
    "            new_evidence.remove(item_to_move)\n",
    "            missing_evidence.append(item_to_move)\n",
    "\n",
    "        new_entry['evidence'] = new_evidence\n",
    "        new_entry['missing_evidence'] = missing_evidence\n",
    "        missing_evidence_data.append(new_entry)\n",
    "\n",
    "    return missing_evidence_data\n",
    "\n",
    "def calculate_wrong_evidence_count(original_count):\n",
    "    \"\"\"\n",
    "    Calculate the number of wrong evidence items to add.\n",
    "    \n",
    "    Args:\n",
    "    original_count (int): The count of original evidence items.\n",
    "    \n",
    "    Returns:\n",
    "    int: The number of wrong evidence items to add.\n",
    "    \"\"\"\n",
    "    base_count = max(1, min(3, original_count // 3))  # At least 1, at most 3\n",
    "    return base_count + (1 if random.random() < 0.3 else 0)  # 30% chance of adding one more\n",
    "\n",
    "def create_wrong_evidence(evidence_kb: List[Dict[str, Any]], exclude_ids: List[int]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create a wrong evidence item that is not in the exclude_ids list.\n",
    "    \n",
    "    Args:\n",
    "    evidence_kb (List[Dict[str, Any]]): The knowledge base of all evidence items.\n",
    "    exclude_ids (List[int]): List of evidence IDs to exclude.\n",
    "    \n",
    "    Returns:\n",
    "    Dict[str, Any]: A wrong evidence item.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        wrong_evidence = random.choice(evidence_kb)\n",
    "        if wrong_evidence['evidence_id'] not in exclude_ids:\n",
    "            return wrong_evidence\n",
    "\n",
    "def create_wrong_evidence_file(baseline_data: List[Dict[str, Any]], evidence_kb: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create a new dataset with wrong evidence items added to each entry.\n",
    "    \n",
    "    Args:\n",
    "    baseline_data (List[Dict[str, Any]]): The original processed assertions data.\n",
    "    evidence_kb (List[Dict[str, Any]]): The knowledge base of all evidence items.\n",
    "    \n",
    "    Returns:\n",
    "    List[Dict[str, Any]]: New dataset with wrong evidence items added.\n",
    "    \"\"\"\n",
    "    wrong_evidence_data = []\n",
    "\n",
    "    for entry in baseline_data:\n",
    "        new_entry = entry.copy()\n",
    "        original_evidence = new_entry['evidence']\n",
    "        original_ids = [item['evidence_id'] for item in original_evidence]\n",
    "        \n",
    "        wrong_evidence_count = calculate_wrong_evidence_count(len(original_evidence))\n",
    "        wrong_evidence = [create_wrong_evidence(evidence_kb, original_ids) for _ in range(wrong_evidence_count)]\n",
    "        \n",
    "        new_entry['wrong_evidence'] = wrong_evidence\n",
    "        wrong_evidence_data.append(new_entry)\n",
    "\n",
    "    return wrong_evidence_data\n",
    "\n",
    "def create_mixed_evidence_file(baseline_data: List[Dict[str, Any]], evidence_kb: List[Dict[str, Any]], deletion_probability: float = 0.5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create a new dataset with some evidence items randomly deleted and wrong evidence items added to each entry.\n",
    "\n",
    "    Args:\n",
    "    baseline_data (List[Dict[str, Any]]): The original processed assertions data.\n",
    "    evidence_kb (List[Dict[str, Any]]): The knowledge base of all evidence items.\n",
    "    deletion_probability (float): Probability of deleting an evidence item (default: 0.5).\n",
    "\n",
    "    Returns:\n",
    "    List[Dict[str, Any]]: New dataset with modified evidence.\n",
    "    \"\"\"\n",
    "    mixed_evidence_data = []\n",
    "\n",
    "    for entry in baseline_data:\n",
    "        new_entry = entry.copy()\n",
    "        original_evidence = new_entry['evidence']\n",
    "        new_evidence = []\n",
    "        missing_evidence = []\n",
    "\n",
    "        # Step 1: Delete some evidence (similar to missing_evidence logic)\n",
    "        num_evidence = len(original_evidence)\n",
    "        if num_evidence > 1:\n",
    "            max_to_delete = random.randint(1, num_evidence - 1)\n",
    "        else:\n",
    "            max_to_delete = 0\n",
    "\n",
    "        for item in original_evidence:\n",
    "            if len(missing_evidence) < max_to_delete and random.random() < deletion_probability:\n",
    "                missing_evidence.append(item)\n",
    "            else:\n",
    "                new_evidence.append(item)\n",
    "\n",
    "        # Ensure at least one item is deleted if possible\n",
    "        if len(missing_evidence) == 0 and len(new_evidence) > 1:\n",
    "            item_to_move = random.choice(new_evidence)\n",
    "            new_evidence.remove(item_to_move)\n",
    "            missing_evidence.append(item_to_move)\n",
    "\n",
    "        # Step 2: Add wrong evidence (similar to wrong_evidence logic)\n",
    "        original_ids = [item['evidence_id'] for item in original_evidence]\n",
    "        wrong_evidence_count = calculate_wrong_evidence_count(len(original_evidence))\n",
    "        wrong_evidence = [create_wrong_evidence(evidence_kb, original_ids) for _ in range(wrong_evidence_count)]\n",
    "\n",
    "        # Update the entry\n",
    "        new_entry['evidence'] = new_evidence\n",
    "        new_entry['missing_evidence'] = missing_evidence\n",
    "        new_entry['wrong_evidence'] = wrong_evidence\n",
    "        mixed_evidence_data.append(new_entry)\n",
    "\n",
    "    return mixed_evidence_data\n",
    "\n",
    "def create_wrong_claim_file(baseline_data: List[Dict[str, Any]], evidence_kb: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create a new dataset with 50 randomly selected claims, each assigned wrong evidence.\n",
    "\n",
    "    Args:\n",
    "    baseline_data (List[Dict[str, Any]]): The original processed assertions data.\n",
    "    evidence_kb (List[Dict[str, Any]]): The knowledge base of all evidence items.\n",
    "\n",
    "    Returns:\n",
    "    List[Dict[str, Any]]: New dataset with wrong claims and evidence.\n",
    "    \"\"\"\n",
    "    wrong_claim_data = []\n",
    "    selected_claims = random.sample(baseline_data, 50)\n",
    "\n",
    "    for entry in selected_claims:\n",
    "        new_entry = {\n",
    "            'claim': entry['claim'],\n",
    "            'explanation': entry['explanation']\n",
    "        }\n",
    "        \n",
    "        original_ids = [item['evidence_id'] for item in entry['evidence']]\n",
    "        wrong_evidence_count = random.randint(3, 7)  # Random number of wrong evidence items (3 to 7)\n",
    "        \n",
    "        wrong_evidence = []\n",
    "        while len(wrong_evidence) < wrong_evidence_count:\n",
    "            candidate = random.choice(evidence_kb)\n",
    "            if candidate['evidence_id'] not in original_ids and candidate not in wrong_evidence:\n",
    "                wrong_evidence.append(candidate)\n",
    "        \n",
    "        new_entry['evidence'] = wrong_evidence\n",
    "        wrong_claim_data.append(new_entry)\n",
    "\n",
    "    return wrong_claim_data\n",
    "\n",
    "def create_assignment_test_file(baseline_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create a new dataset with 50 examples, each containing two random claims and evidence from a third claim.\n",
    "\n",
    "    Args:\n",
    "    baseline_data (List[Dict[str, Any]]): The original processed assertions data.\n",
    "\n",
    "    Returns:\n",
    "    List[Dict[str, Any]]: New dataset for assignment testing.\n",
    "    \"\"\"\n",
    "    assignment_test_data = []\n",
    "\n",
    "    for _ in range(50):\n",
    "        claims = random.sample(baseline_data, 3)\n",
    "        \n",
    "        example = {\n",
    "            'claim_A': claims[0]['claim'],\n",
    "            'claim_B': claims[1]['claim'],\n",
    "            'explanation_A': claims[0]['explanation'],\n",
    "            'explanation_B': claims[1]['explanation'],\n",
    "            'evidence_A': claims[0]['evidence'],\n",
    "            'evidence_B': claims[1]['evidence'],\n",
    "            'evidence_C': claims[2]['evidence']\n",
    "        }\n",
    "        \n",
    "        assignment_test_data.append(example)\n",
    "\n",
    "    return assignment_test_data\n",
    "\n",
    "def get_molecular_profile_summary(molecular_profiles, molecular_profile_id):\n",
    "    \"\"\"\n",
    "    Get the summary for a given molecular profile ID.\n",
    "\n",
    "    Args:\n",
    "    molecular_profiles (pd.DataFrame): DataFrame containing molecular profile data.\n",
    "    molecular_profile_id (int): ID of the molecular profile.\n",
    "\n",
    "    Returns:\n",
    "    str: Summary of the molecular profile, or empty string if not found.\n",
    "    \"\"\"\n",
    "    matching_profiles = molecular_profiles[\n",
    "        molecular_profiles['molecular_profile_id'] == molecular_profile_id\n",
    "    ]\n",
    "    if matching_profiles.empty:\n",
    "        return \"\"\n",
    "    summary = matching_profiles['summary'].iloc[0]\n",
    "    return summary if not pd.isna(summary) else \"\"\n",
    "\n",
    "def create_context(row, molecular_profiles):\n",
    "    \"\"\"\n",
    "    Create a context dictionary from a row of assertion data.\n",
    "\n",
    "    Args:\n",
    "    row (pd.Series): A row from the assertions DataFrame.\n",
    "    molecular_profiles (pd.DataFrame): DataFrame containing molecular profile data.\n",
    "\n",
    "    Returns:\n",
    "    dict: Context information for the assertion.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"Molecular Profile\": row.get('molecular_profile', \"\"),\n",
    "        \"Molecular Profile Summary\": get_molecular_profile_summary(\n",
    "            molecular_profiles, row.get('molecular_profile_id')\n",
    "        ),\n",
    "        \"Disease\": row.get('disease', \"\"),\n",
    "        \"Therapies\": (\", \".join(row['therapies'].split(','))\n",
    "                      if pd.notna(row.get('therapies')) else \"\"),\n",
    "        \"Phenotypes\": (\", \".join(row['phenotypes'].split(','))\n",
    "                       if pd.notna(row.get('phenotypes')) else \"\")\n",
    "    }\n",
    "\n",
    "\n",
    "def add_context_to_file(assertions, molecular_profiles, input_file, output_file):\n",
    "    \"\"\"\n",
    "    Add context to each item in the input file and save to the output file.\n",
    "\n",
    "    Args:\n",
    "    assertions (pd.DataFrame): DataFrame containing assertion data.\n",
    "    molecular_profiles (pd.DataFrame): DataFrame containing molecular profile data.\n",
    "    input_file (str): Path to the input JSON file.\n",
    "    output_file (str): Path to save the output JSON file.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    for item in data:\n",
    "        assertion_row = assertions[\n",
    "            assertions['assertion_summary'] == item['claim']\n",
    "        ].iloc[0]\n",
    "        item['context'] = create_context(assertion_row, molecular_profiles)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "def create_selection_test_file(baseline_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create a new dataset with 50 examples, each containing two random claims and one evidence set with explanation.\n",
    "\n",
    "    Args:\n",
    "    baseline_data (List[Dict[str, Any]]): The original processed assertions data.\n",
    "\n",
    "    Returns:\n",
    "    List[Dict[str, Any]]: New dataset for selection testing.\n",
    "    \"\"\"\n",
    "    selection_test_data = []\n",
    "\n",
    "    for _ in range(50):\n",
    "        claims = random.sample(baseline_data, 2)\n",
    "        \n",
    "        example = {\n",
    "            'claim_A': claims[0]['claim'],\n",
    "            'claim_B': claims[1]['claim'],\n",
    "            'explanation_A': claims[0]['explanation'],\n",
    "            'evidence_A': claims[0]['evidence']\n",
    "        }\n",
    "        \n",
    "        selection_test_data.append(example)\n",
    "\n",
    "    return selection_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Save Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction completed. Files saved in the selected paths.\n"
     ]
    }
   ],
   "source": [
    "# Read necessary files\n",
    "assertions = read_tsv('01-Aug-2024-AssertionSummaries.tsv')\n",
    "clinical_evidence = read_tsv('01-Aug-2024-ClinicalEvidenceSummaries.tsv')\n",
    "molecular_profiles = read_tsv('01-Aug-2024-MolecularProfileSummaries.tsv')\n",
    "\n",
    "# Process assertions\n",
    "processed_assertions = []\n",
    "unique_evidence_items = {}\n",
    "\n",
    "for _, row in assertions.iterrows():\n",
    "    processed_assertion = process_assertion(row, clinical_evidence)\n",
    "    processed_assertions.append(processed_assertion)\n",
    "\n",
    "    # Collect unique evidence items\n",
    "    for evidence in processed_assertion['evidence']:\n",
    "        unique_evidence_items[evidence['evidence_id']] = {\n",
    "            'evidence_id': evidence['evidence_id'],\n",
    "            'description': evidence['description']\n",
    "        }\n",
    "\n",
    "# Save processed assertions\n",
    "save_json(processed_assertions, 'no_context/baseline.json')\n",
    "\n",
    "# Save unique evidence items\n",
    "save_json(list(unique_evidence_items.values()), 'evidence_kb.json')\n",
    "\n",
    "print(\"Data extraction completed. Files saved in the selected paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Evidence File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing evidence file created and saved as 'missing_evidence.json'\n"
     ]
    }
   ],
   "source": [
    "# Load the baseline data\n",
    "with open(CLEAN_DIR / 'no_context/baseline.json', 'r') as f:\n",
    "    baseline_data = json.load(f)\n",
    "\n",
    "# Create the missing evidence dataset\n",
    "missing_evidence_data = create_missing_evidence_file(baseline_data)\n",
    "\n",
    "# Save the missing evidence dataset\n",
    "save_json(missing_evidence_data, 'no_context/missing_evidence.json')\n",
    "\n",
    "print(\"Missing evidence file created and saved as 'missing_evidence.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrong Evidence File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong evidence file created and saved as 'wrong_evidence.json'\n"
     ]
    }
   ],
   "source": [
    "# Load the evidence knowledge base\n",
    "with open(CLEAN_DIR / 'evidence_kb.json', 'r') as f:\n",
    "    evidence_kb = json.load(f)\n",
    "\n",
    "# Create the wrong evidence dataset\n",
    "wrong_evidence_data = create_wrong_evidence_file(baseline_data, evidence_kb)\n",
    "\n",
    "# Save the wrong evidence dataset\n",
    "save_json(wrong_evidence_data, 'no_context/wrong_evidence.json')\n",
    "\n",
    "print(\"Wrong evidence file created and saved as 'wrong_evidence.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed evidence file created and saved as 'mixed.json'\n"
     ]
    }
   ],
   "source": [
    "# Create the mixed evidence dataset\n",
    "mixed_evidence_data = create_mixed_evidence_file(baseline_data, evidence_kb)\n",
    "\n",
    "# Save the mixed evidence dataset\n",
    "save_json(mixed_evidence_data, 'no_context/mixed.json')\n",
    "\n",
    "print(\"Mixed evidence file created and saved as 'mixed.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrong Claim File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong claim file created and saved as 'wrong_claim.json'\n"
     ]
    }
   ],
   "source": [
    "# Create and save wrong claim file\n",
    "wrong_claim_data = create_wrong_claim_file(baseline_data, evidence_kb)\n",
    "save_json(wrong_claim_data, 'no_context/wrong_claim.json')\n",
    "\n",
    "print(\"Wrong claim file created and saved as 'wrong_claim.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment test file created and saved as 'assignment_test.json'\n"
     ]
    }
   ],
   "source": [
    "# Create and save assignment test file\n",
    "assignment_test_data = create_assignment_test_file(baseline_data)\n",
    "save_json(assignment_test_data, 'no_context/assignment_test.json')\n",
    "\n",
    "print(\"Assignment test file created and saved as 'assignment_test.json'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection test file created and saved as 'selection_test.json'\n"
     ]
    }
   ],
   "source": [
    "# Create and save selection test file\n",
    "selection_test_data = create_selection_test_file(baseline_data)\n",
    "save_json(selection_test_data, 'no_context/selection_test.json')\n",
    "\n",
    "print(\"Selection test file created and saved as 'selection_test.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Context to All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created context version of baseline.json\n",
      "Created context version of missing_evidence.json\n",
      "Created context version of wrong_evidence.json\n",
      "Created context version of mixed.json\n",
      "Created context version of wrong_claim.json\n",
      "Created context version of assignment_test.json\n",
      "Created context version of selection_test.json\n",
      "All context files have been created in the 'context' folder.\n",
      "All context files have been created in the 'context' folder.\n"
     ]
    }
   ],
   "source": [
    "# List of files to process\n",
    "files_to_process = [\n",
    "    'baseline.json',\n",
    "    'missing_evidence.json',\n",
    "    'wrong_evidence.json',\n",
    "    'mixed.json',\n",
    "    'wrong_claim.json'\n",
    "]\n",
    "\n",
    "# Process each file\n",
    "for file in files_to_process:\n",
    "    input_file = CLEAN_DIR / 'no_context' / file\n",
    "    output_file = CONTEXT_DIR / file\n",
    "    add_context_to_file(assertions, molecular_profiles, input_file, output_file)\n",
    "    print(f\"Created context version of {file}\")\n",
    "\n",
    "# Special handling for assignment_test.json\n",
    "with open(CLEAN_DIR / 'no_context' / 'assignment_test.json', 'r') as f:\n",
    "    assignment_test_data = json.load(f)\n",
    "\n",
    "for item in assignment_test_data:\n",
    "    assertion_row_A = assertions[\n",
    "        assertions['assertion_summary'] == item['claim_A']\n",
    "    ].iloc[0]\n",
    "    assertion_row_B = assertions[\n",
    "        assertions['assertion_summary'] == item['claim_B']\n",
    "    ].iloc[0]\n",
    "    item['context_A'] = create_context(assertion_row_A, molecular_profiles)\n",
    "    item['context_B'] = create_context(assertion_row_B, molecular_profiles)\n",
    "\n",
    "with open(CONTEXT_DIR / 'assignment_test.json', 'w') as f:\n",
    "    json.dump(assignment_test_data, f, indent=2)\n",
    "\n",
    "print(\"Created context version of assignment_test.json\")\n",
    "\n",
    "# Special handling for selection_test.json\n",
    "with open(CLEAN_DIR / 'no_context' / 'selection_test.json', 'r') as f:\n",
    "    selection_test_data = json.load(f)\n",
    "\n",
    "for item in selection_test_data:\n",
    "    assertion_row_A = assertions[\n",
    "        assertions['assertion_summary'] == item['claim_A']\n",
    "    ].iloc[0]\n",
    "    assertion_row_B = assertions[\n",
    "        assertions['assertion_summary'] == item['claim_B']\n",
    "    ].iloc[0]\n",
    "    item['context_A'] = create_context(assertion_row_A, molecular_profiles)\n",
    "    item['context_B'] = create_context(assertion_row_B, molecular_profiles)\n",
    "\n",
    "with open(CONTEXT_DIR / 'selection_test.json', 'w') as f:\n",
    "    json.dump(selection_test_data, f, indent=2)\n",
    "\n",
    "print(\"Created context version of selection_test.json\")\n",
    "\n",
    "print(\"All context files have been created in the 'context' folder.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
